# Estructura de Data Science y ML Pipeline

Este directorio contiene toda la infraestructura de Data Science y Machine Learning del proyecto.

## ðŸ“Š Estructura de Datos

```
data/
â”œâ”€â”€ raw/                    # Datos originales (sin modificar)
â”‚   â”œâ”€â”€ keylogger_dataset_small.csv   # ~38k registros (15MB)
â”‚   â””â”€â”€ keylogger_dataset_large.csv   # ~500k registros (298MB)
â”œâ”€â”€ processed/              # Datos limpios y procesados
â”œâ”€â”€ external/               # Datos externos o de terceros
â””â”€â”€ features/               # Features engineering results
```

## ðŸ§ª Notebooks de ExploraciÃ³n

```
notebooks/
â”œâ”€â”€ 01_exploratory_data_analysis.ipynb    # EDA inicial
â”œâ”€â”€ 02_data_cleaning.ipynb                # Limpieza de datos
â”œâ”€â”€ 03_feature_engineering.ipynb          # IngenierÃ­a de caracterÃ­sticas
â”œâ”€â”€ 04_model_training.ipynb               # Entrenamiento de modelos
â””â”€â”€ 05_model_evaluation.ipynb             # EvaluaciÃ³n y mÃ©tricas
```

## ðŸ¤– Pipeline de ML

```
ml_pipeline/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_model.py              # Script de entrenamiento
â”‚   â”œâ”€â”€ feature_engineering.py      # Pipeline de features
â”‚   â””â”€â”€ data_preprocessing.py       # Preprocesamiento
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate_model.py           # EvaluaciÃ³n de modelos
â”‚   â”œâ”€â”€ cross_validation.py         # ValidaciÃ³n cruzada
â”‚   â””â”€â”€ metrics_calculation.py      # CÃ¡lculo de mÃ©tricas
â””â”€â”€ deployment/
    â”œâ”€â”€ model_converter.py          # ConversiÃ³n PKL â†’ ONNX
    â””â”€â”€ model_validator.py          # ValidaciÃ³n de modelos
```

## ðŸŽ¯ Modelos Entrenados

```
models/
â”œâ”€â”€ development/            # Modelos en desarrollo
â”œâ”€â”€ production/             # Modelos en producciÃ³n
â”œâ”€â”€ experiments/            # Experimentos y A/B testing
â””â”€â”€ metadata/              # Metadatos y configuraciones
```

## ðŸš€ Flujo de Trabajo

### 1. ExploraciÃ³n de Datos
```bash
jupyter notebook notebooks/01_exploratory_data_analysis.ipynb
```

### 2. Limpieza de Datos
```bash
python ml_pipeline/training/data_preprocessing.py
```

### 3. Entrenamiento
```bash
python ml_pipeline/training/train_model.py --dataset processed/clean_dataset.parquet
```

### 4. EvaluaciÃ³n
```bash
python ml_pipeline/evaluation/evaluate_model.py --model models/latest/model.pkl
```

### 5. ConversiÃ³n y Deployment
```bash
python ml_pipeline/deployment/model_converter.py --input models/latest/model.pkl
```

## ðŸ“ˆ Mejoras Propuestas

### Formato de Datos
- **De CSV a Parquet**: Mejor compresiÃ³n y velocidad
- **De CSV a DuckDB**: Consultas SQL rÃ¡pidas
- **Particionamiento**: Para datasets grandes

### Algoritmos ML
- **XGBoost**: Para mejor rendimiento
- **LightGBM**: Para datasets grandes
- **AutoML**: OptimizaciÃ³n automÃ¡tica
- **Deep Learning**: Redes neuronales para detecciÃ³n avanzada

### MLOps
- **MLflow**: Tracking de experimentos
- **DVC**: Versionado de datos
- **Docker**: ContainerizaciÃ³n de modelos
- **CI/CD**: Pipeline automatizado