{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32516a0",
   "metadata": {},
   "source": [
    "# üîç An√°lisis Exploratorio de Datos - Anti-Keylogger Dataset\n",
    "\n",
    "Este notebook realiza un an√°lisis exploratorio completo de los datasets de detecci√≥n de keyloggers, incluyendo limpieza de datos, an√°lisis estad√≠stico y preparaci√≥n para machine learning.\n",
    "\n",
    "## üìä Objetivos del An√°lisis\n",
    "\n",
    "1. **Explorar** los datasets raw (38k y 500k registros)\n",
    "2. **Limpiar** datos inconsistentes y valores faltantes\n",
    "3. **Analizar** distribuciones y patrones\n",
    "4. **Preparar** datos para modelos ML avanzados\n",
    "5. **Exportar** en formatos eficientes (Parquet, HDF5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c3ee4",
   "metadata": {},
   "source": [
    "## 1. üìö Import Required Libraries\n",
    "\n",
    "Importamos las bibliotecas esenciales para data science, visualizaci√≥n y machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad9963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bibliotecas importadas exitosamente\n",
      "üìÖ Fecha de an√°lisis: 2025-09-18 11:40:47\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Data format handlers\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas exitosamente\")\n",
    "print(f\"üìÖ Fecha de an√°lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa0dbb",
   "metadata": {},
   "source": [
    "## 2. üìÇ Load and Explore Raw Datasets\n",
    "\n",
    "Cargamos los datasets raw y exploramos su estructura b√°sica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09d6f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cargando datasets...\n",
      "‚úÖ Dataset peque√±o cargado: 38,998 filas, 86 columnas\n",
      "‚úÖ Dataset peque√±o cargado: 38,998 filas, 86 columnas\n",
      "‚úÖ Muestra del dataset grande cargada: 50,000 filas, 86 columnas\n",
      "‚úÖ Muestra del dataset grande cargada: 50,000 filas, 86 columnas\n",
      "üìä Dataset grande completo: 523,617 filas estimadas\n",
      "üìä Dataset grande completo: 523,617 filas estimadas\n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "DATA_DIR = Path(\"../data/raw\")\n",
    "PROCESSED_DIR = Path(\"../data/processed\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "print(\"üîÑ Cargando datasets...\")\n",
    "\n",
    "try:\n",
    "    # Load small dataset (~38k records)\n",
    "    df_small = pd.read_csv(DATA_DIR / \"keylogger_dataset_small.csv\", low_memory=False)\n",
    "    print(f\"‚úÖ Dataset peque√±o cargado: {df_small.shape[0]:,} filas, {df_small.shape[1]} columnas\")\n",
    "    \n",
    "    # Load large dataset (~500k records) - sample first to avoid memory issues\n",
    "    df_large_sample = pd.read_csv(DATA_DIR / \"keylogger_dataset_large.csv\", nrows=50000, low_memory=False)\n",
    "    print(f\"‚úÖ Muestra del dataset grande cargada: {df_large_sample.shape[0]:,} filas, {df_large_sample.shape[1]} columnas\")\n",
    "    \n",
    "    # Get basic info about large dataset without loading it all\n",
    "    with open(DATA_DIR / \"keylogger_dataset_large.csv\", 'r') as f:\n",
    "        line_count = sum(1 for line in f) - 1  # Subtract header\n",
    "    print(f\"üìä Dataset grande completo: {line_count:,} filas estimadas\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Aseg√∫rate de que los archivos est√©n en la carpeta data/raw/\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inesperado: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
